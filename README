--> Prerequisites
All source files are written in python3. The following external libraries
are required : numpy, matplotlib, pickle and ntlk.
Also, nltk.corpus' set of stop words need to be downloaded using :
>>> nltk.download()
in any python shell, if you don't have it already.

--> Groundwork
Preprocess.py is used to split given data in Dataset.dat into training and
testing datasets, and to preprocess the data into a machine readable form.
The output of these files are two text files - Testingtset.txt and
Trainingset.txt, along with python objects dict.pkl, trvec.pkl and tsvec.pkl.
(Dictionary, and 1-hot input vectors. Instead of generating the vector before
each iteration, they are loaded once, and used repeatedly by all the parts.)
Since these files are large, they won't be submitted. Instead use :
$ python Preprocess.py
to generate them locally. The total size is nearly 300 MiB.

FeedForwardNN.py defines a class that models a generic
feedforward neural network. This is used in both parts.
To test this code, and to provide example usage of this class, two example
programs CrossEntropyExample.py and SquaredErrorExample.py are also given.
$ python *Example.py
These programs generate a random 2D dataset and use a neural network to learn
its decision boundary. They intermittently keep showing how the model
classifies a small subset of the Euclidean plane into blue and red classes.

--> Solutions to assignments
Part A has only one output neuron, and uses squared error loss function.
A1.py uses logistic activation. Test outputs 'spam' / 'ham'
must be 1 / 0 respectively. Output was generated by :
$ python A1.py 15000 100 A1.png A1.log

A2.py uses tanh activation. Accordingly, the 'spam' / 'ham'
values must be +1/-1 respectively. Output was generated by :
$ python A2.py 7000 100 A2.png A2.log

B.py also uses tanh activation in the hidden layer. The output layer has no
activation function. It's directly fed to softmax layer that produces
probabilities. Cross entropy loss is used to make the gradient
linear in terms of network output and given labels. Output was generated by :
$ python B.py 10000 100 B.png B.log

--> Checking
Since all the parts required to plot the loss function with iterations,
the commands issued had the 2nd argument set to a relatively small number.
To check the code, if plots are not needed, you can use :
$ python A2.py 7000 100000 test.png test.log
to just run SGD and get the final accuracy. It shouldn't take more
than 2 minutes on any modern terminal. Just generating the plots
takes most of the time.
